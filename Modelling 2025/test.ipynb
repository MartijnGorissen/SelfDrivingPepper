{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from obj_model.pt...\n",
      "Model loaded successfully!\n",
      "Model is ready for inference.\n",
      "Performing inference on: Data 2025/Stoplicht groen + rood/1715347328.1574562.png\n",
      "\n",
      "image 1/1 c:\\Users\\larsl\\Downloads\\SDC\\Data 2025\\Stoplicht groen + rood\\1715347354.3135037.png: 384x640 2 zebras, 1 rood, 92.3ms\n",
      "Speed: 3.2ms preprocess, 92.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Inference complete. Processing results...\n",
      "Displaying image with detections (press any key to close)...\n",
      "Saving annotated image to: output_image.png\n",
      "Detection summary:\n",
      "An error occurred during loading or inference: 'Results' object has no attribute 'print'. See valid attributes below.\n",
      "\n",
      "    A class for storing and manipulating inference results.\n",
      "\n",
      "    This class provides methods for accessing, manipulating, and visualizing inference results from various\n",
      "    Ultralytics models, including detection, segmentation, classification, and pose estimation.\n",
      "\n",
      "    Attributes:\n",
      "        orig_img (numpy.ndarray): The original image as a numpy array.\n",
      "        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n",
      "        boxes (Boxes | None): Detected bounding boxes.\n",
      "        masks (Masks | None): Segmentation masks.\n",
      "        probs (Probs | None): Classification probabilities.\n",
      "        keypoints (Keypoints | None): Detected keypoints.\n",
      "        obb (OBB | None): Oriented bounding boxes.\n",
      "        speed (dict): Dictionary containing inference speed information.\n",
      "        names (dict): Dictionary mapping class indices to class names.\n",
      "        path (str): Path to the input image file.\n",
      "        save_dir (str | None): Directory to save results.\n",
      "\n",
      "    Methods:\n",
      "        update: Updates the Results object with new detection data.\n",
      "        cpu: Returns a copy of the Results object with all tensors moved to CPU memory.\n",
      "        numpy: Converts all tensors in the Results object to numpy arrays.\n",
      "        cuda: Moves all tensors in the Results object to GPU memory.\n",
      "        to: Moves all tensors to the specified device and dtype.\n",
      "        new: Creates a new Results object with the same image, path, names, and speed attributes.\n",
      "        plot: Plots detection results on an input RGB image.\n",
      "        show: Displays the image with annotated inference results.\n",
      "        save: Saves annotated inference results image to file.\n",
      "        verbose: Returns a log string for each task in the results.\n",
      "        save_txt: Saves detection results to a text file.\n",
      "        save_crop: Saves cropped detection images to specified directory.\n",
      "        summary: Converts inference results to a summarized dictionary.\n",
      "        to_df: Converts detection results to a Pandas Dataframe.\n",
      "        to_json: Converts detection results to JSON format.\n",
      "        to_csv: Converts detection results to a CSV format.\n",
      "        to_xml: Converts detection results to XML format.\n",
      "        to_html: Converts detection results to HTML format.\n",
      "        to_sql: Converts detection results to an SQL-compatible format.\n",
      "\n",
      "    Examples:\n",
      "        >>> results = model(\"path/to/image.jpg\")\n",
      "        >>> result = results[0]  # Get the first result\n",
      "        >>> boxes = result.boxes  # Get the boxes for the first result\n",
      "        >>> masks = result.masks  # Get the masks for the first result\n",
      "        >>> for result in results:\n",
      "        >>>     result.plot()  # Plot detection results\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2 # Import OpenCV if you want to load images manually first\n",
    "import torch # Might be needed, good to import\n",
    "\n",
    "# --- Load the model (as you did successfully before) ---\n",
    "file_path = 'obj_model.pt'\n",
    "print(f\"Loading model from {file_path}...\")\n",
    "try:\n",
    "    # Load using the YOLO class - this worked for you\n",
    "    model = YOLO(file_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(\"Model is ready for inference.\")\n",
    "\n",
    "    # --- Perform Inference ---\n",
    "    # Option A: Provide the path directly to the model (simplest)\n",
    "    image_path = 'linksafrecht.jpg' # <<< CHANGE THIS to your image path\n",
    "    red = 'Data 2025/Stoplicht groen + rood/1715347354.3135037.png' # <<< CHANGE THIS to your image path\n",
    "    green = 'Data 2025/Stoplicht groen + rood/1715347328.1574562.png' # <<< CHANGE THIS to your image path\n",
    "    print(f\"Performing inference on: {green}\")\n",
    "\n",
    "    # The model automatically handles loading the image, preprocessing, inference, and postprocessing\n",
    "    # It will run on the CPU by default since you loaded it onto the CPU.\n",
    "    results = model(red)\n",
    "    # or use model.predict(): results = model.predict(image_path)\n",
    "\n",
    "    # Option B: Load image with OpenCV first (if you need the numpy array)\n",
    "    # print(f\"Loading image with OpenCV: {image_path}\")\n",
    "    # img_np = cv2.imread(image_path)\n",
    "    # if img_np is None:\n",
    "    #     print(f\"Error: Could not read image at {image_path}\")\n",
    "    # else:red\n",
    "    #     print(\"Performing inference on image loaded via OpenCV...\")\n",
    "    #     # Pass the NumPy array (BGR format from cv2 is handled automatically)\n",
    "    #     results = model(img_np)\n",
    "\n",
    "    # --- Process Results ---\n",
    "    # 'results' is usually a list containing one 'Results' object per input image.\n",
    "    if results:\n",
    "        print(\"Inference complete. Processing results...\")\n",
    "        first_result = results[0] # Get results for the first (and likely only) image\n",
    "\n",
    "        # 1. Show the image with detections drawn on it\n",
    "        #    This will open a display window (requires a GUI environment)\n",
    "        print(\"Displaying image with detections (press any key to close)...\")\n",
    "        first_result.show()\n",
    "\n",
    "        # 2. Save the image with detections to a file\n",
    "        output_image_path = 'output_image.png'\n",
    "        print(f\"Saving annotated image to: {output_image_path}\")\n",
    "        first_result.save(output_image_path)\n",
    "\n",
    "        # 3. Print a summary of the detections to the console\n",
    "        print(\"Detection summary:\")\n",
    "        first_result.print()\n",
    "\n",
    "        # 4. Access raw detection data (more advanced)\n",
    "        # boxes = first_result.boxes  # Detection boxes object\n",
    "        # for box in boxes:\n",
    "        #     class_id = int(box.cls)\n",
    "        #     confidence = float(box.conf)\n",
    "        #     coordinates = box.xyxy[0] # xyxy format\n",
    "        #     print(f\" Class: {model.names[class_id]}, Confidence: {confidence:.2f}, Coords: {coordinates}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Inference completed, but no results were returned.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle cases where the model file or the image file isn't found\n",
    "    print(f\"Error: File not found. Check paths for model ('{file_path}') and image ('{image_path}')\")\n",
    "except ImportError:\n",
    "     print(\"ImportError occurred. Ensure 'ultralytics' and 'opencv-python' are installed (`pip install ultralytics opencv-python`).\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading or inference: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from traffic-lights.pt...\n",
      "Model loaded successfully!\n",
      "Model is ready for inference.\n",
      "Performing inference on: Data 2025/Stoplicht groen + rood/1715347328.1574562.png\n",
      "\n",
      "image 1/1 c:\\Users\\larsl\\Downloads\\SDC\\Data 2025\\Stoplicht groen + rood\\1715347328.1574562.png: 384x640 1 GreenCircular, 392.9ms\n",
      "Speed: 2.2ms preprocess, 392.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Inference complete. Processing results...\n",
      "Displaying image with detections (press any key to close)...\n",
      "Saving annotated image to: output_image.png\n",
      "Detection summary:\n",
      "An error occurred during loading or inference: 'Results' object has no attribute 'print'. See valid attributes below.\n",
      "\n",
      "    A class for storing and manipulating inference results.\n",
      "\n",
      "    This class provides methods for accessing, manipulating, and visualizing inference results from various\n",
      "    Ultralytics models, including detection, segmentation, classification, and pose estimation.\n",
      "\n",
      "    Attributes:\n",
      "        orig_img (numpy.ndarray): The original image as a numpy array.\n",
      "        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n",
      "        boxes (Boxes | None): Detected bounding boxes.\n",
      "        masks (Masks | None): Segmentation masks.\n",
      "        probs (Probs | None): Classification probabilities.\n",
      "        keypoints (Keypoints | None): Detected keypoints.\n",
      "        obb (OBB | None): Oriented bounding boxes.\n",
      "        speed (dict): Dictionary containing inference speed information.\n",
      "        names (dict): Dictionary mapping class indices to class names.\n",
      "        path (str): Path to the input image file.\n",
      "        save_dir (str | None): Directory to save results.\n",
      "\n",
      "    Methods:\n",
      "        update: Updates the Results object with new detection data.\n",
      "        cpu: Returns a copy of the Results object with all tensors moved to CPU memory.\n",
      "        numpy: Converts all tensors in the Results object to numpy arrays.\n",
      "        cuda: Moves all tensors in the Results object to GPU memory.\n",
      "        to: Moves all tensors to the specified device and dtype.\n",
      "        new: Creates a new Results object with the same image, path, names, and speed attributes.\n",
      "        plot: Plots detection results on an input RGB image.\n",
      "        show: Displays the image with annotated inference results.\n",
      "        save: Saves annotated inference results image to file.\n",
      "        verbose: Returns a log string for each task in the results.\n",
      "        save_txt: Saves detection results to a text file.\n",
      "        save_crop: Saves cropped detection images to specified directory.\n",
      "        summary: Converts inference results to a summarized dictionary.\n",
      "        to_df: Converts detection results to a Pandas Dataframe.\n",
      "        to_json: Converts detection results to JSON format.\n",
      "        to_csv: Converts detection results to a CSV format.\n",
      "        to_xml: Converts detection results to XML format.\n",
      "        to_html: Converts detection results to HTML format.\n",
      "        to_sql: Converts detection results to an SQL-compatible format.\n",
      "\n",
      "    Examples:\n",
      "        >>> results = model(\"path/to/image.jpg\")\n",
      "        >>> result = results[0]  # Get the first result\n",
      "        >>> boxes = result.boxes  # Get the boxes for the first result\n",
      "        >>> masks = result.masks  # Get the masks for the first result\n",
      "        >>> for result in results:\n",
      "        >>>     result.plot()  # Plot detection results\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2 # Import OpenCV if you want to load images manually first\n",
    "import torch # Might be needed, good to import\n",
    "\n",
    "# --- Load the model (as you did successfully before) ---\n",
    "file_path = 'traffic-lights.pt'\n",
    "print(f\"Loading model from {file_path}...\")\n",
    "try:\n",
    "    # Load using the YOLO class - this worked for you\n",
    "    model = YOLO(file_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(\"Model is ready for inference.\")\n",
    "\n",
    "    # --- Perform Inference ---\n",
    "    # Option A: Provide the path directly to the model (simplest)\n",
    "    red = 'Data 2025/Stoplicht groen + rood/1715347354.3135037.png' # <<< CHANGE THIS to your image path\n",
    "    green = 'Data 2025/Stoplicht groen + rood/1715347328.1574562.png'\n",
    "    print(f\"Performing inference on: {image_path}\")\n",
    "\n",
    "    # The model automatically handles loading the image, preprocessing, inference, and postprocessing\n",
    "    # It will run on the CPU by default since you loaded it onto the CPU.\n",
    "    results = model(image_path)\n",
    "    # or use model.predict(): results = model.predict(image_path)\n",
    "\n",
    "    # Option B: Load image with OpenCV first (if you need the numpy array)\n",
    "    # print(f\"Loading image with OpenCV: {image_path}\")\n",
    "    # img_np = cv2.imread(image_path)\n",
    "    # if img_np is None:\n",
    "    #     print(f\"Error: Could not read image at {image_path}\")\n",
    "    # else:\n",
    "    #     print(\"Performing inference on image loaded via OpenCV...\")\n",
    "    #     # Pass the NumPy array (BGR format from cv2 is handled automatically)\n",
    "    #     results = model(img_np)\n",
    "\n",
    "    # --- Process Results ---\n",
    "    # 'results' is usually a list containing one 'Results' object per input image.\n",
    "    if results:\n",
    "        print(\"Inference complete. Processing results...\")\n",
    "        first_result = results[0] # Get results for the first (and likely only) image\n",
    "\n",
    "        # 1. Show the image with detections drawn on it\n",
    "        #    This will open a display window (requires a GUI environment)\n",
    "        print(\"Displaying image with detections (press any key to close)...\")\n",
    "        first_result.show()\n",
    "\n",
    "        # 2. Save the image with detections to a file\n",
    "        output_image_path = 'output_image.png'\n",
    "        print(f\"Saving annotated image to: {output_image_path}\")\n",
    "        first_result.save(output_image_path)\n",
    "\n",
    "        # 3. Print a summary of the detections to the console\n",
    "        print(\"Detection summary:\")\n",
    "        first_result.print()\n",
    "\n",
    "        # 4. Access raw detection data (more advanced)\n",
    "        # boxes = first_result.boxes  # Detection boxes object\n",
    "        # for box in boxes:\n",
    "        #     class_id = int(box.cls)\n",
    "        #     confidence = float(box.conf)\n",
    "        #     coordinates = box.xyxy[0] # xyxy format\n",
    "        #     print(f\" Class: {model.names[class_id]}, Confidence: {confidence:.2f}, Coords: {coordinates}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Inference completed, but no results were returned.\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle cases where the model file or the image file isn't found\n",
    "    print(f\"Error: File not found. Check paths for model ('{file_path}') and image ('{image_path}')\")\n",
    "except ImportError:\n",
    "     print(\"ImportError occurred. Ensure 'ultralytics' and 'opencv-python' are installed (`pip install ultralytics opencv-python`).\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading or inference: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: obj_model.pt\n",
      "Model loaded successfully!\n",
      "------------------------------\n",
      "Inspecting Model Parameters:\n",
      "------------------------------\n",
      "Number of Classes (nc): 6\n",
      "Class Names (ID: Name):\n",
      "  0: zebra\n",
      "  1: rood\n",
      "  2: groen\n",
      "  3: mens\n",
      "  4: snelheidsbord\n",
      "  5: auto\n",
      "Detected Input Image Size (imgsz): 640\n",
      "Model Task: detect\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- Load the model ---\n",
    "file_path = 'obj_model.pt'\n",
    "print(f\"Loading model from: {file_path}\")\n",
    "\n",
    "try:\n",
    "    model = YOLO(file_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Inspecting Model Parameters:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 1. Get Class Labels (Names) and Number of Classes (nc)\n",
    "    if hasattr(model, 'names'):\n",
    "        class_names = model.names\n",
    "        num_classes = len(class_names)\n",
    "        print(f\"Number of Classes (nc): {num_classes}\")\n",
    "        print(\"Class Names (ID: Name):\")\n",
    "        # Print neatly - model.names is usually a dict {id: name}\n",
    "        for class_id, class_name in class_names.items():\n",
    "            print(f\"  {class_id}: {class_name}\")\n",
    "    else:\n",
    "        print(\"Class names (model.names) not found in the loaded model metadata.\")\n",
    "        class_names = None\n",
    "        num_classes = None\n",
    "\n",
    "    # 2. Try to Get Input Image Size (imgsz)\n",
    "    # This is often stored in the model's arguments if saved during/after ultralytics training\n",
    "    input_size = None\n",
    "    if hasattr(model, 'args'):\n",
    "         # Check common places where imgsz might be stored\n",
    "        if hasattr(model.args, 'imgsz'):\n",
    "            input_size = model.args.imgsz\n",
    "        elif isinstance(model.args, dict) and 'imgsz' in model.args:\n",
    "            input_size = model.args['imgsz']\n",
    "\n",
    "    if input_size:\n",
    "        print(f\"Detected Input Image Size (imgsz): {input_size}\")\n",
    "    else:\n",
    "        print(\"Input Image Size (imgsz) not explicitly found in model.args metadata.\")\n",
    "        print(\"  (Note: Ultralytics often defaults to 640x640 during inference if not specified.\")\n",
    "        print(\"   The actual required input tensor shape might differ based on architecture.)\")\n",
    "\n",
    "\n",
    "    # 3. Get the Task (Detection, Segmentation, Classification)\n",
    "    if hasattr(model, 'task'):\n",
    "        print(f\"Model Task: {model.task}\")\n",
    "    else:\n",
    "        print(\"Model task attribute not found.\")\n",
    "\n",
    "    # 4. Explore other potential attributes (Optional)\n",
    "    # print(\"\\n--- Other available attributes via dir(model): ---\")\n",
    "    # print(dir(model))\n",
    "    # print(\"\\n--- Model arguments if available (model.args): ---\")\n",
    "    # if hasattr(model, 'args'):\n",
    "    #     print(vars(model.args) if hasattr(model.args, '__dict__') else model.args)\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file not found at {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading or inspection: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\larsl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model from: obj_model.pt\n",
      "Model loaded successfully.\n",
      "Starting training...\n",
      "New https://pypi.org/project/ultralytics/8.3.105 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.97  Python-3.11.1 torch-2.6.0+cpu CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=obj_model.pt, data=data.yaml, epochs=50, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=finetune_run_with_new_objects2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\finetune_run_with_new_objects2\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\larsl\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 755k/755k [00:00<00:00, 18.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=6 with nc=9\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753067  ultralytics.nn.modules.head.Detect           [9, [64, 128, 256]]           \n",
      "YOLOv8n summary: 129 layers, 3,012,603 parameters, 3,012,587 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\finetune_run_with_new_objects2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\larsl\\Downloads\\SDC\\Modelling 2025\\datasets\\dataset\\train\\labels... 750 images, 2 backgrounds, 0 corrupt: 100%|██████████| 750/750 [00:01<00:00, 545.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\larsl\\Downloads\\SDC\\Modelling 2025\\datasets\\dataset\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\larsl\\Downloads\\SDC\\Modelling 2025\\datasets\\dataset\\val\\labels... 68 images, 0 backgrounds, 0 corrupt: 100%|██████████| 68/68 [00:00<00:00, 421.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\larsl\\Downloads\\SDC\\Modelling 2025\\datasets\\dataset\\val\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\finetune_run_with_new_objects2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000769, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\finetune_run_with_new_objects2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      2.699      5.848      1.917         29        640:   4%|▍         | 4/94 [00:18<07:01,  4.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetune_run_with_new_objects\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Optional: Name for the output folder\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_yaml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_to_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device=0  # Uncomment to force GPU usage (if available, 0 is usually the first GPU)\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Other arguments like learning rate (lr0), optimizer can be added if needed,\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# but defaults are often okay for fine-tuning.\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# workers=4 # Adjust based on CPU cores for data loading\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Print the location of saved results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\larsl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:791\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\larsl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:211\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\larsl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:392\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    388\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    389\u001b[0m     )\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 392\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32mc:\\Users\\larsl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\larsl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\larsl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch # Good practice to import\n",
    "\n",
    "# 1. Load your existing model\n",
    "#    This tells YOLO to start training from these weights.\n",
    "model_path = 'obj_model.pt'\n",
    "print(f\"Loading pre-trained model from: {model_path}\")\n",
    "model = YOLO(model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# 2. Define path to your dataset configuration YAML file\n",
    "data_yaml_path = 'data.yaml' # <<< UPDATE THIS PATH\n",
    "\n",
    "# 3. Define training arguments\n",
    "epochs_to_train = 50 # Adjust as needed. Start with fewer (e.g., 25-50) for fine-tuning.\n",
    "image_size = 640     # Use the image size your model was originally trained on, if known, or a standard size like 640.\n",
    "batch_size = 8       # Adjust based on your GPU memory (e.g., 4, 8, 16). Lower if you get memory errors.\n",
    "run_name = 'finetune_run_with_new_objects' # Optional: Name for the output folder\n",
    "\n",
    "print(\"Starting training...\")\n",
    "results = model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=epochs_to_train,\n",
    "    imgsz=image_size,\n",
    "    batch=batch_size,\n",
    "    name=run_name,\n",
    "    # device=0  # Uncomment to force GPU usage (if available, 0 is usually the first GPU)\n",
    "    # Other arguments like learning rate (lr0), optimizer can be added if needed,\n",
    "    # but defaults are often okay for fine-tuning.\n",
    "    # workers=4 # Adjust based on CPU cores for data loading\n",
    ")\n",
    "print(\"Training finished.\")\n",
    "print(f\"Results saved to: {results.save_dir}\") # Print the location of saved results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
